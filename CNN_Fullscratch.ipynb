{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN_Fullscratch\n",
    "### フルスクラッチでCNNを実装し、CIFER10画像認識タスクに適用する   \n",
    "### ネットワーク構造：LeNet\n",
    "### データ：CIFER10 (Keras) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from keras.datasets import cifar10\n",
    "% matplotlib inline\n",
    "image_num = 50000\n",
    "\n",
    "(x_train_cifar10, y_train_cifar10), (x_test_cifar10, y_test_cifar10) = cifar10.load_data()\n",
    "\n",
    "x_train_cifar10 = x_train_cifar10[:image_num]\n",
    "y_train_cifar10 = y_train_cifar10[:image_num]\n",
    "y_label_cifar10 = y_train_cifar10\n",
    "y_train_cifar10 = np.identity(10)[y_train_cifar10]\n",
    "\n",
    "del x_test_cifar10, y_test_cifar10\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, params={}):\n",
    "        if 'input_shape' in params:\n",
    "            self.in_shape = params['input_shape']\n",
    "        else:\n",
    "            self.in_shape = None\n",
    "\n",
    "        if 'output_shape' in params:\n",
    "            self.out_shape = params['output_shape']\n",
    "        else:\n",
    "            self.out_shape = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MaxPooling_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(Layer):\n",
    "    \n",
    "    def __init__(self, pool_size=4, stride=-1, pad=0, params={}):\n",
    "        super(MaxPooling, self).__init__(params)\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.padding = 'constant'\n",
    "        self.stride = self.pool_size if self.stride == -1 else stride\n",
    "        self.max_index = None\n",
    "        self.input_shape = None \n",
    "\n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape\n",
    "        N, H, W, C = in_shape\n",
    "        self.out_h = 1 + int((H + 2 * self.pad - self.pool_size) // self.stride)\n",
    "        self.out_w = 1 + int((W + 2 * self.pad - self.pool_size) // self.stride)\n",
    "        self.out_shape = (N, self.out_h, self.out_w, C)\n",
    "        print(\"Pooling : out {} filter {} stride {} pad {} \".format(self.out_shape, \\\n",
    "                                                                    self.pool_size, \\\n",
    "                                                                    self.stride,\\\n",
    "                                                                    self.pad))\n",
    "        return self.out_shape\n",
    "\n",
    "   \n",
    "    def forward(self, x):\n",
    "        if self.pad > 0:\n",
    "            x = np.pad(x, [(0, 0), (self.pad, self.pad), (self.pad, self.pad), (0, 0)], self.padding)\n",
    "        self.input_shape = x.shape\n",
    "        N, H, W, C = x.shape\n",
    "        col = np.array([x[n, h: h + self.pool_size, w: w + self.pool_size, c].flatten() \\\n",
    "                       for c in range(C) \\\n",
    "                       for n in range(N) \\\n",
    "                       for h in range(0, H - self.pool_size + 1, self.stride) \\\n",
    "                       for w in range(0, W - self.pool_size + 1, self.stride)])\n",
    "        col_max = np.max(col, axis=1)\n",
    "        self.max_index = np.argmax(col, axis=1)\n",
    "        return col_max.reshape(C, N, self.out_h, self.out_w).transpose(1, 2, 3, 0)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        filter_size = dout.shape[1]\n",
    "        dout_line = dout.transpose(3, 0, 1, 2).reshape(-1)\n",
    "        dout = np.zeros([dout_line.shape[0], self.pool_size * self.pool_size])\n",
    "        for i, max_i in enumerate(self.max_index):\n",
    "            dout[i, max_i] = dout_line[i]\n",
    "        in_N, in_H, in_W, in_C = self.input_shape\n",
    "        img = np.zeros((in_N, in_H + 2 * self.pad, in_W + 2 * self.pad, in_C))\n",
    "        for i, line in enumerate(dout):\n",
    "            channel_i, data_i = divmod(i, in_N * self.out_h * self.out_w)\n",
    "            data_i, square_i = divmod(data_i, self.out_h * self.out_w)\n",
    "            height_i, width_i = divmod(square_i, self.out_h)\n",
    "            height_i *= self.stride\n",
    "            width_i *= self.stride\n",
    "            img[data_i, height_i: height_i + self.pool_size, width_i:width_i + self.pool_size, channel_i] = \\\n",
    "            line.reshape(self.pool_size, self.pool_size)\n",
    "        return img[:, self.pad : self.pad + in_H, self.pad : self.pad + in_W, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \n",
    "    def __init__(self, out_channel=1, filter_size=3, stride=1, pad=0, bias=True, params={}):\n",
    "        self.filnum = out_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.filsize = filter_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.bias = bias\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.x = None\n",
    "        self.padding = 'constant'\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.optimize = None\n",
    "        if 'lr' in params:\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "        self.x_2dim = None\n",
    "\n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape  \n",
    "        N, H, W, in_channel = in_shape\n",
    "        self.h_num = 1 + ((H + 2 * self.pad - self.filter_size) // self.stride)\n",
    "        self.w_num = 1 + ((W + 2 * self.pad - self.filter_size) // self.stride)\n",
    "        self.out_shape = (N, self.h_num, self.w_num, self.out_channel)\n",
    "        self.W = np.random.randn(self.out_channel, (self.filter_size **2) * in_channel) * 0.1\n",
    "        self.b = np.random.randn(self.filnum,1)\n",
    "        self.db = np.zeros((self.filnum,1))\n",
    "        \n",
    "        if 'optimizer' in params:\n",
    "            if params['optimizer'] == 'sgd':\n",
    "                self.optimize = self.update_sgd\n",
    "            elif params['optimizer'] == 'adagrad':\n",
    "                self.h = np.zeros_like(W)\n",
    "                self.optimize = self.update_adagrad\n",
    "            else:  # params['optimizer'] == 'adam':\n",
    "                self.m = np.zeros_like(self.W)\n",
    "                self.v = np.zeros_like(self.W)\n",
    "                self.beta1 = 0.9\n",
    "                self.beta2 = 0.999\n",
    "                self.optimize = self.update_adam\n",
    "        else:\n",
    "            self.optimize = self.update_sgd\n",
    "            \n",
    "        print(\"Convolution : out {}, filter {}, stride {} \".format(self.out_shape, \\\n",
    "                                                                   self.filter_size,\\\n",
    "                                                                   self.stride))\n",
    "        return self.out_shape\n",
    "    \n",
    "    # im2col\n",
    "    def forward(self, x):\n",
    "        if self.pad > 0:\n",
    "            x = np.pad(x, [(0, 0), (self.pad, self.pad), (self.pad, self.pad), (0, 0)], self.padding)\n",
    "        self.x = x   \n",
    "        N, H, W, C = x.shape\n",
    "        self.x_col = np.array([self.x[n, h: h + self.filsize, w: w + self.filsize, :].reshape(-1)\\\n",
    "                              for n in range(N)\\\n",
    "                              for h in range(0, H - self.filsize + 1, self.stride)\\\n",
    "                              for w in range(0, W - self.filsize + 1, self.stride)])\n",
    "        return  (np.dot(self.W, self.x_col.T) + self.b).transpose(1, 0)\\\n",
    "                .reshape(N, self.h_num, self.w_num, self.filnum)\n",
    "    # col2im\n",
    "    def backward(self, dout): \n",
    "        N, H, W, C = self.x.shape\n",
    "        dout1 = dout.reshape(N * self.h_num * self.w_num, self.filnum)                 \n",
    "        dx0 = np.dot(dout1, self.W)\n",
    "        dx = np.zeros(self.x.shape)\n",
    "        for h in range(self.h_num):\n",
    "            for w in range(self.w_num):\n",
    "                dx[:, h: h + self.filsize, w: w + self.filsize, :] += dx0.reshape(N, \\\n",
    "                                                                                   self.h_num, \\\n",
    "                                                                                   self.w_num, \\\n",
    "                                                                                   C * (self.filsize**2))\\\n",
    "                                                                                   [:, h, w, :]\\\n",
    "                                                                                   .reshape(N, \\\n",
    "                                                                                            self.filsize, \\\n",
    "                                                                                            self.filsize, \\\n",
    "                                                                                            C)\n",
    "        self.dW = np.dot(dout1.T, self.x_col)\n",
    "        self.db = dout1.T.sum(axis=1, keepdims=True)\n",
    "        return dx[:, self.pad:self.pad + self.in_shape[1], self.pad:self.pad + self.in_shape[2], :]\n",
    "\n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "\n",
    "    def update_adagrad(self, lr=0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "\n",
    "    def update_adam(self, lr=0.01):\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (self.dW * self.dW)\n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    def current_weight(self):\n",
    "        return self.W, self.b\n",
    "    \n",
    "    def set_weight(self, w, b):\n",
    "        self.W = w.reshape(self.W.shape)\n",
    "        self.b = b.reshape(self.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "\n",
    "    def __init__(self, params={}):\n",
    "        super(Flatten, self).__init__(params)\n",
    "\n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape\n",
    "        N, H, W, C = in_shape\n",
    "        self.out_shape = (N, H * W * C)\n",
    "        print(\"Flatten : out {} \".format(self.out_shape))\n",
    "        return self.out_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        out = np.array([elem.flatten() for elem in x])\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.reshape(self.input_shape)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, params):\n",
    "        super(Dropout, self).__init__(params)\n",
    "        if 'dropout_ratio' in params:\n",
    "            self.dropout_ratio = params['dropout_ratio']\n",
    "        else:\n",
    "            self.dropout_ratio = 0.5\n",
    "        self.mask = None\n",
    "    \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = self.in_shape\n",
    "        print(\"Dropout : ratio {}  \".format(self.dropout_ratio))\n",
    "        return self.out_shape\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.random_sample(x.shape) > self.dropout_ratio \n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNormalization_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, params):\n",
    "        super(BatchNorm, self).__init__(params)\n",
    "        self.out = None\n",
    "        self.beta = 0.0\n",
    "        self.gamma = 1.0\n",
    "        self.lr = params['lr']\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = self.in_shape\n",
    "        print(\"BatchNorm : out {} lr {}  \".format(self.out_shape, self.lr))\n",
    "        return self.out_shape\n",
    "\n",
    "    def forward(self, x):   \n",
    "        # step1\n",
    "        mu = np.mean(x, axis=0)\n",
    "        # step2\n",
    "        self.xmu = x - mu\n",
    "        # step3\n",
    "        sq = self.xmu ** 2\n",
    "        # step4\n",
    "        self.var = np.var(x, axis=0)\n",
    "        # step5\n",
    "        self.sqrtvar = np.sqrt(self.var + self.eps)\n",
    "        # step6\n",
    "        self.ivar = 1.0 / self.sqrtvar\n",
    "        # step7\n",
    "        self.xhat = self.xmu * self.ivar\n",
    "        # step8\n",
    "        gammax = self.gamma * self.xhat\n",
    "        # step9\n",
    "        out = gammax + self.beta\n",
    "        return out\n",
    " \n",
    "    def backward(self, dout=1):\n",
    "        N, D = dout.shape\n",
    "        \n",
    "        # step9\n",
    "        self.d_beta = np.sum(dout, axis=0)\n",
    "        dgammax = dout  \n",
    "        # step8\n",
    "        self.d_gamma = np.sum(dgammax * self.xhat, axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "        # step7\n",
    "        divar = np.sum(dxhat * self.xmu, axis=0)\n",
    "        dxmu1 = dxhat * self.ivar\n",
    "        # step6\n",
    "        dsqrtvar = -1. / (self.sqrtvar ** 2) * divar\n",
    "        # step5\n",
    "        dvar = 0.5 * 1. / np.sqrt(self.var + self.eps) * dsqrtvar\n",
    "        # step4\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        # step3\n",
    "        dxmu2 = 2 * self.xmu * dsq\n",
    "        # step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        # step1\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "        # step0\n",
    "        dx = dx1 + dx2\n",
    "        return dx\n",
    "   \n",
    "    def optimize(self):\n",
    "        self.gamma -= self.lr * self.d_gamma\n",
    "        self.beta -= self.lr * self.d_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super(Activation, self).__init__(params)\n",
    "        self.out = None\n",
    "        self.mask = None\n",
    "        if 'activation' in params:\n",
    "            if params['activation'] == 'tanh':\n",
    "                self.forward = self.forward_tanh\n",
    "                self.backward = self.backward_tanh\n",
    "            elif params['activation'] == 'sigmoid':\n",
    "                self.forward = self.forward_sigmoid\n",
    "                self.backward = self.backward_sigmoid\n",
    "            else:  # params['activation'] == 'relu':\n",
    "                self.forward = self.forward_relu\n",
    "                self.backward = self.backward_relu\n",
    "        else:\n",
    "            params['activation'] = 'relu'\n",
    "            self.forward = self.forward_relu\n",
    "            self.backward = self.backward_relu\n",
    "\n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = self.in_shape\n",
    "        print(\"Activation : out {}   func : {} \".format(self.out_shape, params['activation']))\n",
    "        return self.out_shape\n",
    "\n",
    "    \n",
    "    # ReLU\n",
    "    def forward_relu(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "  \n",
    "    def backward_relu(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "\n",
    "    # tanh\n",
    "    def forward_tanh(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward_tanh(self, dout):\n",
    "        dx = dout * (1 - np.tanh(dout) ** 2)\n",
    "        return dx\n",
    "\n",
    "    # sigmoid\n",
    "    def forward_sigmoid(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward_sigmoid(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(Layer):\n",
    "    \n",
    "    def __init__(self, unit_size=100, params={}):\n",
    "        super(Affine, self).__init__(params)\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.x = None\n",
    "        self.unit_size = unit_size\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        if 'lr' in params:\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "            \n",
    "    def initialize(self, in_shape, params={}):\n",
    "        self.in_shape = in_shape\n",
    "        N, F = in_shape\n",
    "        self.out_shape = (N, self.unit_size)\n",
    "\n",
    "        # initialize\n",
    "        self.W = np.random.randn(F, self.unit_size)\n",
    "        self.b = np.zeros([1, self.unit_size])\n",
    "        self.W *= 0.01\n",
    "\n",
    "        # optimizer\n",
    "        if 'optimizer' in params:\n",
    "            if params['optimizer'] == 'sgd':\n",
    "                self.optimize = self.update_sgd\n",
    "            elif params['optimizer'] == 'adagrad':\n",
    "                self.h = np.zeros_like(self.W)\n",
    "                self.optimize = self.update_adagrad\n",
    "            else:  # params['optimizer'] == 'adam':\n",
    "                self.m = np.zeros_like(self.W)\n",
    "                self.v = np.zeros_like(self.W)\n",
    "                self.beta1 = 0.9\n",
    "                self.beta2 = 0.999\n",
    "                self.optimize = self.update_adam\n",
    "        else:\n",
    "            params['optimizer'] = 'adam'\n",
    "            self.optimize = self.update_adam\n",
    "        print(\"Affine : out {} optimizer {} unit {} \".format(self.out_shape, params['optimizer'],\\\n",
    "                                                             self.unit_size))\n",
    "        return self.out_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "    \n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "        \n",
    "    def update_adagrad(self, lr=0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "     \n",
    "    def update_adam(self, lr=0.01):\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (self.dW * self.dW)\n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    def current_weight(self):\n",
    "        return self.W, self.b\n",
    "    \n",
    "    def set_weight(self, w, b):\n",
    "        self.W = w.reshape(self.W.shape)\n",
    "        self.b = b.reshape(self.b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftmaxWithLoss_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_error(y, y_pred):\n",
    "    data_size = y.shape[0]\n",
    "    cross_entorpy = -np.sum(y * np.log(y_pred + 1e-7))\n",
    "    error = cross_entorpy / data_size\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(Layer):\n",
    "    def __init__(self, params={}):\n",
    "        super(SoftmaxWithLoss, self).__init__(params)\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNetLayers_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetLayers:\n",
    "    def __init__(self, params):\n",
    "        unit_size_list = [params['input_size']]\n",
    "        unit_size_list.extend(params['hidden_layer_list'])\n",
    "        unit_size_list.append(params['output_size'])\n",
    "\n",
    "        self.params = {}\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(6, 5, 1, 2) \n",
    "        self.layers['Active1'] = Activation(params)\n",
    "        self.layers['Pool1'] = MaxPooling(2, 2, 0, params)\n",
    "        self.layers['Conv2'] = Convolution(16, 5, 1, 2)\n",
    "        self.layers['Active2'] = Activation(params)\n",
    "        self.layers['Pool2'] = MaxPooling(2, 2, 0, params)\n",
    "        self.layers['Flatten'] = Flatten(params)\n",
    "        self.layers['Affine1'] = Affine(120, params)\n",
    "        self.layers['Dropout1'] = Dropout(params)\n",
    "        self.layers['BatchNorm1'] = BatchNorm(params)\n",
    "        self.layers['Active3'] = Activation(params)\n",
    "        self.layers['Affine2'] = Affine(84, params)\n",
    "        self.layers['Dropout2'] = Dropout(params)\n",
    "        self.layers['BatchNorm2'] = BatchNorm(params)\n",
    "        self.layers['Active4'] = Activation(params)\n",
    "        self.layers['Affine3'] = Affine(params['output_size'], params)\n",
    "     \n",
    "    def initialize(self, x, y, params):\n",
    "        in_shape = x.shape \n",
    "        print(\"Input_shape:\",x.shape)\n",
    "        out_shape = y.shape \n",
    "        print('###########' * 5)\n",
    "        for i, layer_ in enumerate(self.layers.values()):\n",
    "            print(\" Layer {}\".format(i))\n",
    "            in_shape = layer_.initialize(in_shape, params)\n",
    "            print('###########' * 5)\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(t, axis=1)\n",
    "        data_size = x.shape[0]\n",
    "        correct_count = np.sum([y_true == y_pred])\n",
    "        score = correct_count / data_size * 100\n",
    "        return round(score, 2)\n",
    " \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    " \n",
    "    def optimize(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)   \n",
    "        # backward\n",
    "        dout = self.lastLayer.backward(1)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)    \n",
    "        for layer in self.layers.values():         \n",
    "            if hasattr(layer, \"optimize\"):\n",
    "                layer.optimize()\n",
    "    \n",
    "    def collect_weight(self):\n",
    "        weight_list=[]\n",
    "        for layer in self.layers.values():\n",
    "            if hasattr(layer, \"current_weight\"):\n",
    "                w, b = layer.current_weight()\n",
    "                weight_list.append(w.flatten())\n",
    "                weight_list.append(b.flatten())\n",
    "        return pd.DataFrame(weight_list) \n",
    "    \n",
    "    def reuse_weight(self, w):\n",
    "        i = 0\n",
    "        for layer in self.layers.values():\n",
    "            if hasattr(layer, \"set_weight\"):\n",
    "                layer.set_weight(w[i][~np.isnan(w[i])], \\\n",
    "                                 w[i+1][~np.isnan(w[i+1])])\n",
    "                i += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, init='gauss', epochs=10, lr=0.05, lam=0.01,\n",
    "                 batch_mode='mini', activation='relu',\n",
    "                 batch_size=32, hidden_layer_list=[20,20], optimizer='sgd',\n",
    "                 batch_norm=False, dropout_ratio=0.3, save_weight=None,\n",
    "                 use_weight=None):\n",
    "        \n",
    "        \"\"\" ハイパーパラメータ\n",
    "        init: 初期化手法\n",
    "            'he'\n",
    "            'gauss' \n",
    "            'xavier'\n",
    "        lr : 学習率\n",
    "        lam : 正則化項の率\n",
    "        batch_mode: 学習モード\n",
    "            'batch'\n",
    "            'mini'\n",
    "            'online'\n",
    "        hidden_layer_list : 隠れ層のリスト、層のユニットをリストで入力\n",
    "        optimizer : 勾配更新手法\n",
    "            'sgd'\n",
    "            'adam'\n",
    "            'adagrad'\n",
    "        act_func: 活性化関数\n",
    "            'relu'\n",
    "            'tanh'\n",
    "            'sigmoid'\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['epochs'] = epochs\n",
    "        self.params['init'] = init\n",
    "        self.params['lr'] = lr\n",
    "        self.params['lam'] = lam  \n",
    "        self.params['batch_mode'] = batch_mode  \n",
    "        self.params['batch_size'] = batch_size\n",
    "        self.params['hidden_layer_list'] = hidden_layer_list\n",
    "        self.params['optimizer'] = optimizer\n",
    "        self.params['batch_norm'] = batch_norm\n",
    "        self.params['dropout_ratio'] = dropout_ratio\n",
    "        self.params['activation'] = activation\n",
    "        self.params['save_weight'] = save_weight\n",
    "        self.params['use_weight'] = use_weight\n",
    "        \n",
    "\n",
    "    def train(self, X, y, params={}):\n",
    "        for key in params:\n",
    "            self.params[key] = params[key]\n",
    "\n",
    "        X = X / 255.0\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.params['data_size'] = X_train.shape[0]\n",
    "        self.params['input_size'] = X_train.shape[1]\n",
    "        self.params['output_size'] = y_train.shape[1]\n",
    "\n",
    "        past_train_costs = []\n",
    "        past_test_costs = []\n",
    "        past_train_accuracy = []\n",
    "        past_test_accuracy = []\n",
    "\n",
    "        N, H, W, C = X_train.shape\n",
    "\n",
    "        # 学習モード\n",
    "        if self.params['batch_mode'] == 'batch':\n",
    "            self.params['batch_size'] = self.params['data_size']\n",
    "        elif self.params['batch_mode'] == 'online':\n",
    "            self.params['batch_size'] = 1\n",
    "        \n",
    "        # 隠れ層やレイヤーインスタンス生成\n",
    "        in_shape = (self.params['batch_size'], H, W, C)\n",
    "        self.params['layer'] = LeNetLayers(self.params)\n",
    "        # 入出力サイズ\n",
    "        self.params['layer'].initialize(X_train[:self.params['batch_size']],\\\n",
    "                                        y_train[:self.params['batch_size']],\\\n",
    "                                        self.params)\n",
    "        # 学習済みweightファイルから値を取得\n",
    "        if self.params['use_weight'] is not None:\n",
    "            past_weight = pd.read_csv(self.params['use_weight'])\n",
    "            self.params['layer'].reuse_weight(np.array(past_weight))\n",
    "\n",
    "        iteration = int(X_train.shape[0] / self.params['batch_size'])\n",
    "        choice_list = list(range(X_train.shape[0]))\n",
    "        \n",
    "        for i in range(self.params['epochs']*iteration):\n",
    "            start = i % iteration * self.params['batch_size']\n",
    "            end = start + self.params['batch_size']\n",
    "            X_batch, y_batch = X_train[choice_list[start:end]], y_train[choice_list[start:end]]\n",
    "            \n",
    "            # 勾配を求め値を更新  \n",
    "            self.params['layer'].optimize(X_batch, y_batch)\n",
    "            # 正答率とコストを算出して保存\n",
    "            if i==0 or (i%iteration is 0):\n",
    "                # 正答率とコストを算出して保存\n",
    "                train_acc = self.params['layer'].accuracy(X_train, y_train)\n",
    "                past_train_accuracy.append(train_acc)\n",
    "                test_acc = self.params['layer'].accuracy(X_test, y_test)\n",
    "                past_test_accuracy.append(test_acc)\n",
    "                train_loss = self.params['layer'].loss(X_train, y_train)\n",
    "                past_train_costs.append(train_loss)\n",
    "                test_loss = self.params['layer'].loss(X_test, y_test)\n",
    "                past_test_costs.append(test_loss)\n",
    "                print(\"epoch:{} train_acc:{}, train_loss:{}, test_acc:{}, test_loss:{}\"\\\n",
    "                      .format(i//iteration, train_acc, train_loss, test_acc, test_loss))\n",
    "                #wegiht保存  \n",
    "                if self.params['save_weight'] is not None:\n",
    "                    weights = self.params['layer'].collect_weight()\n",
    "                    weights.to_csv(self.params['save_weight']\\\n",
    "                                   + '_' + str(i//iteration)\\\n",
    "                                   + '_' + str(round(test_acc, 1))\\\n",
    "                                   + '_' + str(round(test_loss, 2))\\\n",
    "                                   + '.csv', index=False)\n",
    "\n",
    "            if i%iteration is 9:\n",
    "                random.shuffle(choice_list)\n",
    "        return past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs\n",
    "\n",
    "\n",
    "    def predict(self, X, probability=False):\n",
    "        predict = self.params['layer'].predict(X, train_flg=False)\n",
    "        predict_proba = softmax(predict)\n",
    "        if probability is True:\n",
    "            return predict_proba\n",
    "        else:\n",
    "            return np.argmax(predict_proba, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_shape: (32, 32, 32, 3)\n",
      "#######################################################\n",
      " Layer 0\n",
      "Convolution : out (32, 32, 32, 6), filter 5, stride 1 \n",
      "#######################################################\n",
      " Layer 1\n",
      "Activation : out (32, 32, 32, 6)   func : relu \n",
      "#######################################################\n",
      " Layer 2\n",
      "Pooling : out (32, 16, 16, 6) filter 2 stride 2 pad 0 \n",
      "#######################################################\n",
      " Layer 3\n",
      "Convolution : out (32, 16, 16, 16), filter 5, stride 1 \n",
      "#######################################################\n",
      " Layer 4\n",
      "Activation : out (32, 16, 16, 16)   func : relu \n",
      "#######################################################\n",
      " Layer 5\n",
      "Pooling : out (32, 8, 8, 16) filter 2 stride 2 pad 0 \n",
      "#######################################################\n",
      " Layer 6\n",
      "Flatten : out (32, 1024) \n",
      "#######################################################\n",
      " Layer 7\n",
      "Affine : out (32, 120) optimizer adam unit 120 \n",
      "#######################################################\n",
      " Layer 8\n",
      "Dropout : ratio 0.3  \n",
      "#######################################################\n",
      " Layer 9\n",
      "BatchNorm : out (32, 120) lr 0.01  \n",
      "#######################################################\n",
      " Layer 10\n",
      "Activation : out (32, 120)   func : relu \n",
      "#######################################################\n",
      " Layer 11\n",
      "Affine : out (32, 84) optimizer adam unit 84 \n",
      "#######################################################\n",
      " Layer 12\n",
      "Dropout : ratio 0.3  \n",
      "#######################################################\n",
      " Layer 13\n",
      "BatchNorm : out (32, 84) lr 0.01  \n",
      "#######################################################\n",
      " Layer 14\n",
      "Activation : out (32, 84)   func : relu \n",
      "#######################################################\n",
      " Layer 15\n",
      "Affine : out (32, 10) optimizer adam unit 10 \n",
      "#######################################################\n",
      "epoch:0 train_acc:56.5, train_loss:9.119812663909702, test_acc:53.06, test_loss:9.379169314309458\n",
      "epoch:1 train_acc:57.18, train_loss:9.087571453828396, test_acc:54.4, test_loss:9.396946735567488\n",
      "epoch:2 train_acc:60.8, train_loss:8.71884296470338, test_acc:56.8, test_loss:9.110109576215214\n",
      "epoch:3 train_acc:62.2, train_loss:8.376622426467387, test_acc:57.83, test_loss:8.807788156389764\n",
      "epoch:4 train_acc:62.9, train_loss:8.189238381044792, test_acc:58.46, test_loss:8.685542735976037\n",
      "epoch:5 train_acc:63.93, train_loss:8.03538836000364, test_acc:58.62, test_loss:8.554419717034285\n",
      "epoch:6 train_acc:64.55, train_loss:7.964856120511304, test_acc:59.09, test_loss:8.573040964723484\n",
      "epoch:7 train_acc:65.18, train_loss:7.765935365825567, test_acc:59.6, test_loss:8.405525492882926\n",
      "epoch:8 train_acc:65.4, train_loss:7.7278570653885765, test_acc:59.01, test_loss:8.465825099570177\n",
      "epoch:9 train_acc:66.17, train_loss:7.696942204597064, test_acc:58.98, test_loss:8.427788418834641\n",
      "epoch:10 train_acc:67.51, train_loss:7.421028664107565, test_acc:59.67, test_loss:8.216266182660672\n",
      "epoch:11 train_acc:67.59, train_loss:7.392068096662792, test_acc:59.66, test_loss:8.25612909927701\n",
      "epoch:12 train_acc:67.83, train_loss:7.29885026596894, test_acc:60.11, test_loss:8.145721156541109\n",
      "epoch:13 train_acc:68.17, train_loss:7.264494505191725, test_acc:59.8, test_loss:8.14693711633386\n",
      "epoch:14 train_acc:68.92, train_loss:7.260880585929152, test_acc:60.07, test_loss:8.205009720445089\n",
      "epoch:15 train_acc:68.72, train_loss:7.154722075477389, test_acc:59.68, test_loss:8.074841578834853\n",
      "epoch:16 train_acc:69.21, train_loss:7.104096168673334, test_acc:60.42, test_loss:8.073420962034199\n",
      "epoch:17 train_acc:69.58, train_loss:6.990665618148909, test_acc:59.77, test_loss:8.0557138163472\n",
      "epoch:18 train_acc:69.36, train_loss:6.916497210851078, test_acc:59.77, test_loss:8.000644247811385\n",
      "epoch:19 train_acc:69.55, train_loss:6.916940049511122, test_acc:60.54, test_loss:8.006987973202174\n",
      "last train cost is 6.916940049511122\n",
      "last test cost is 8.006987973202174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'iteration[epoch]')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN(epochs=20, optimizer='adam',batch_norm=True)\n",
    "# weightファイルを読み込み途中から学習再開\n",
    "params = {'batch_mode' : 'mini',\n",
    "          'lr': 0.01,\n",
    "          'use_weight':'weights_6_55.2_9.29.csv',\n",
    "          'save_weight':'weights'\n",
    "         }\n",
    "\n",
    "past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs = model.train(x_train_cifar10,\\\n",
    "                                                                                         y_train_cifar10.\\\n",
    "                                                                                         reshape(image_num, 10),\\\n",
    "                                                                                         params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(past_train_costs, color='orange', label='train')\n",
    "plt.plot(past_test_costs, color='lime', label='test')\n",
    "plt.ylabel(\"cost\", fontsize=15)\n",
    "print(\"last train cost is {}\".format(past_train_costs[-1]))\n",
    "print(\"last test cost is {}\".format(past_test_costs[-1]))\n",
    "plt.legend()\n",
    "plt.title('Learning Curve', fontsize=20)\n",
    "plt.xlabel(\"iteration[epoch]\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今回は試験的にシンプルなモデル LeNet（Convolutionレイヤーが２つ）を採用した  \n",
    "#### 更なる性能向上にはVGGやinceptionのようにレイヤー数、ユニット数の増加が必要と考えられる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
